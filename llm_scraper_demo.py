#!/usr/bin/env python3
"""
LLMã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  åŒ…æ‹¬çš„ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥ä¸‹ã‚’å®Ÿæ¼”ã—ã¾ã™ï¼š
1. ç”ŸHTMLãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¨è¡¨ç¤º
2. å‰å‡¦ç†ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã¨è¡¨ç¤º  
3. LLMæŠ½å‡ºçµæœã®è¡¨ç¤º
4. ç•°ãªã‚‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ‰‹æ³•ã®æ¯”è¼ƒ
5. ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚­ãƒ¼ãƒã®ä½¿ç”¨ä¾‹
"""

import asyncio
import os
import sys
import json
import time
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from typing import List, Optional

# ãƒ‘ã‚¹ã®è¨­å®š
sys.path.append(os.path.join(os.path.dirname(__file__), 'banner_maker', 'src'))

from llm_scraper import LLMWebScraper

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

class ProductInfo(BaseModel):
    """ã‚«ã‚¹ã‚¿ãƒ å•†å“æƒ…å ±ã‚¹ã‚­ãƒ¼ãƒï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰"""
    product_name: Optional[str] = Field(description="å•†å“å")
    brand: Optional[str] = Field(description="ãƒ–ãƒ©ãƒ³ãƒ‰å")
    price: Optional[str] = Field(description="ä¾¡æ ¼")
    availability: Optional[str] = Field(description="åœ¨åº«çŠ¶æ³")
    description: Optional[str] = Field(description="å•†å“èª¬æ˜")
    features: Optional[List[str]] = Field(description="ä¸»è¦æ©Ÿèƒ½")
    category: Optional[str] = Field(description="ã‚«ãƒ†ã‚´ãƒª")

def print_section(title: str, content: str = ""):
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¤ºç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
    print("\n" + "="*80)
    print(f"ğŸ“‹ {title}")
    print("="*80)
    if content:
        print(content)

def print_subsection(title: str):
    """ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¤ºç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
    print(f"\nğŸ”¹ {title}")
    print("-" * 60)

def truncate_text(text: str, max_length: int = 200) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šé•·ã•ã§åˆ‡ã‚Šè©°ã‚ã‚‹"""
    if len(text) <= max_length:
        return text
    return text[:max_length] + "..."

async def demonstrate_scraping_process(url: str, site_name: str):
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    print_section(f"{site_name} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", f"URL: {url}")
    
    # API ã‚­ãƒ¼ã®ç¢ºèª
    api_key = os.getenv('GOOGLE_API_KEY')
    if not api_key:
        print("âŒ ã‚¨ãƒ©ãƒ¼: GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   .envãƒ•ã‚¡ã‚¤ãƒ«ã«API ã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„:")
        print("   GOOGLE_API_KEY='your-api-key-here'")
        return

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–
    scraper = LLMWebScraper(api_key)
    
    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: ç”ŸHTMLãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        print_subsection("ã‚¹ãƒ†ãƒƒãƒ—1: ç”ŸHTMLãƒ‡ãƒ¼ã‚¿ã®å–å¾—")
        print("ğŸ”„ HTMLå–å¾—ä¸­...")
        
        start_time = time.time()
        raw_html, method_used = await scraper._get_raw_html_with_method(url)
        fetch_time = time.time() - start_time
        
        if raw_html:
            print(f"âœ… HTMLå–å¾—æˆåŠŸï¼")
            print(f"ğŸ“Š å–å¾—æ–¹æ³•: {method_used}")
            print(f"â±ï¸  å–å¾—æ™‚é–“: {fetch_time:.2f}ç§’")
            print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(raw_html):,} æ–‡å­—")
            print(f"ğŸ” HTMLãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
            print(truncate_text(raw_html, 300))
        else:
            print("âŒ HTMLå–å¾—å¤±æ•—")
            return
            
        # ã‚¹ãƒ†ãƒƒãƒ—2: HTMLã®å‰å‡¦ç†
        print_subsection("ã‚¹ãƒ†ãƒƒãƒ—2: HTMLã®å‰å‡¦ç†ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°")
        print("ğŸ”„ å‰å‡¦ç†å®Ÿè¡Œä¸­...")
        
        start_time = time.time()
        preprocessed_content = scraper._preprocess_html_for_llm(raw_html)
        preprocess_time = time.time() - start_time
        
        print(f"âœ… å‰å‡¦ç†å®Œäº†ï¼")
        print(f"â±ï¸  å‡¦ç†æ™‚é–“: {preprocess_time:.2f}ç§’")
        print(f"ğŸ“ å‡¦ç†å‰ã‚µã‚¤ã‚º: {len(raw_html):,} æ–‡å­—")
        print(f"ğŸ“ å‡¦ç†å¾Œã‚µã‚¤ã‚º: {len(preprocessed_content):,} æ–‡å­—")
        print(f"ğŸ“‰ åœ§ç¸®ç‡: {((len(raw_html) - len(preprocessed_content)) / len(raw_html) * 100):.1f}%")
        print(f"ğŸ” å‰å‡¦ç†å¾Œãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
        print(truncate_text(preprocessed_content, 400))
        
        # å‰å‡¦ç†ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        preprocessed_file = scraper._save_preprocessed_data(url, preprocessed_content)
        if preprocessed_file:
            print(f"ğŸ’¾ å‰å‡¦ç†ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {preprocessed_file}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–æŠ½å‡º
        print_subsection("ã‚¹ãƒ†ãƒƒãƒ—3: LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡º")
        print("ğŸ”„ LLMå‡¦ç†ä¸­...")
        
        start_time = time.time()
        llm_result = await scraper._extract_with_llm_structured(
            preprocessed_content, url, ProductInfo
        )
        llm_time = time.time() - start_time
        
        print(f"âœ… LLMæŠ½å‡ºå®Œäº†ï¼")
        print(f"â±ï¸  LLMå‡¦ç†æ™‚é–“: {llm_time:.2f}ç§’")
        print(f"ğŸ¯ ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: {llm_result.get('extraction_confidence', 0):.2f}")
        print(f"ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {llm_result.get('model_used', 'unknown')}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: æŠ½å‡ºçµæœã®è©³ç´°è¡¨ç¤º
        print_subsection("ã‚¹ãƒ†ãƒƒãƒ—4: æŠ½å‡ºã•ã‚ŒãŸæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿")
        extracted_data = llm_result.get('llm_extracted_data', {})
        
        if extracted_data:
            for key, value in extracted_data.items():
                if value and str(value).strip() not in ['', 'null', 'None', 'N/A']:
                    if isinstance(value, list):
                        print(f"â€¢ {key}: {', '.join(str(v) for v in value[:3])}{'...' if len(value) > 3 else ''}")
                    else:
                        display_value = truncate_text(str(value), 100)
                        print(f"â€¢ {key}: {display_value}")
        else:
            print("âš ï¸  æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: æœ€çµ‚çµæœã®ç”Ÿæˆã¨ä¿å­˜
        print_subsection("ã‚¹ãƒ†ãƒƒãƒ—5: æœ€çµ‚çµæœã®ç”Ÿæˆ")
        
        final_result = {
            'url': url,
            'llm_extracted_data': extracted_data,
            'extraction_method': method_used,
            'confidence': llm_result.get('extraction_confidence', 0.0),
            'timestamp': time.time(),
            'model_used': scraper.model_name,
            'preprocessed_data_file': preprocessed_file,
            'performance_metrics': {
                'html_fetch_time': fetch_time,
                'preprocessing_time': preprocess_time,
                'llm_processing_time': llm_time,
                'total_time': fetch_time + preprocess_time + llm_time,
                'data_compression_ratio': f"{((len(raw_html) - len(preprocessed_content)) / len(raw_html) * 100):.1f}%"
            }
        }
        
        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        result_filename = f"demo_result_{site_name.lower().replace(' ', '_')}.json"
        with open(result_filename, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ’¾ æœ€çµ‚çµæœä¿å­˜: {result_filename}")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼
        total_time = fetch_time + preprocess_time + llm_time
        print_subsection("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼")
        print(f"âš¡ åˆè¨ˆå‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
        print(f"   - HTMLå–å¾—: {fetch_time:.2f}ç§’ ({fetch_time/total_time*100:.1f}%)")
        print(f"   - å‰å‡¦ç†: {preprocess_time:.2f}ç§’ ({preprocess_time/total_time*100:.1f}%)")
        print(f"   - LLMå‡¦ç†: {llm_time:.2f}ç§’ ({llm_time/total_time*100:.1f}%)")
        
        return final_result
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        print(f"ğŸ“ è©³ç´°:")
        traceback.print_exc()
        return None

async def compare_methods():
    """ç•°ãªã‚‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ‰‹æ³•ã®æ¯”è¼ƒãƒ‡ãƒ¢"""
    
    print_section("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ‰‹æ³•æ¯”è¼ƒãƒ‡ãƒ¢")
    
    test_urls = [
        ("https://creditcard-get.net/genre/s008pt-2/?code=loget-002", "ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚«ãƒ¼ãƒ‰æ¯”è¼ƒã‚µã‚¤ãƒˆ"),
        ("https://www.yodobashi.com/product/100000001005807664/", "ãƒ¨ãƒ‰ãƒã‚·ã‚«ãƒ¡ãƒ©å•†å“ãƒšãƒ¼ã‚¸")
    ]
    
    results = []
    
    for url, site_name in test_urls:
        print(f"\nğŸŒ ãƒ†ã‚¹ãƒˆå¯¾è±¡: {site_name}")
        result = await demonstrate_scraping_process(url, site_name)
        if result:
            results.append((site_name, result))
    
    # æ¯”è¼ƒã‚µãƒãƒªãƒ¼
    if len(results) >= 2:
        print_section("æ‰‹æ³•æ¯”è¼ƒã‚µãƒãƒªãƒ¼")
        
        for site_name, result in results:
            method = result['extraction_method']
            confidence = result['confidence']
            total_time = result['performance_metrics']['total_time']
            print(f"ğŸ”¹ {site_name}:")
            print(f"   - ä½¿ç”¨æ‰‹æ³•: {method}")
            print(f"   - ä¿¡é ¼åº¦: {confidence:.2f}")
            print(f"   - å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
            print(f"   - å•†å“å: {result['llm_extracted_data'].get('product_name', 'N/A')}")

async def demonstrate_schema_flexibility():
    """ã‚¹ã‚­ãƒ¼ãƒã®æŸ”è»Ÿæ€§ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ¦‚å¿µèª¬æ˜ã®ã¿ï¼‰"""
    
    print_section("ã‚¹ã‚­ãƒ¼ãƒæŸ”è»Ÿæ€§ã®èª¬æ˜")
    
    print("ğŸ”§ ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚­ãƒ¼ãƒæ©Ÿèƒ½ã«ã¤ã„ã¦:")
    print("   - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚­ãƒ¼ãƒä»¥å¤–ã«ã‚‚ã€ç”¨é€”ã«å¿œã˜ãŸã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©å¯èƒ½")
    print("   - ä¾‹: ä¸å‹•ç”£æƒ…å ±ã€æ±‚äººæƒ…å ±ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãªã©ç‰¹å®šåˆ†é‡ã«ç‰¹åŒ–")
    print("   - Pydanticã‚’ä½¿ç”¨ã—ãŸå‹å®‰å…¨ãªã‚¹ã‚­ãƒ¼ãƒå®šç¾©")
    print("\nğŸ’¡ ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚­ãƒ¼ãƒä¾‹:")
    print("   class RealEstateInfo(BaseModel):")
    print("       property_type: str = Field(description='ç‰©ä»¶ç¨®åˆ¥')")
    print("       price: str = Field(description='ä¾¡æ ¼')")
    print("       location: str = Field(description='æ‰€åœ¨åœ°')")
    print("       floor_area: str = Field(description='åºŠé¢ç©')")
    print("       nearest_station: str = Field(description='æœ€å¯„ã‚Šé§…')")
    print("\nğŸ“ ä½¿ç”¨æ–¹æ³•:")
    print("   result = await scraper.scrape_page_with_llm(")
    print("       url, extraction_schema=RealEstateInfo)")
    print("\nâœ¨ ã“ã®æ©Ÿèƒ½ã«ã‚ˆã‚Šã€æ§˜ã€…ãªåˆ†é‡ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã«å¯¾å¿œå¯èƒ½ã§ã™")

async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°"""
    
    print("ğŸš€ LLMã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  åŒ…æ‹¬çš„ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*80)
    print("ã“ã®ãƒ‡ãƒ¢ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®Ÿæ¼”ã—ã¾ã™:")
    print("1. ç”ŸHTMLãƒ‡ãƒ¼ã‚¿ã®å–å¾—")
    print("2. HTMLã®å‰å‡¦ç†ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°")
    print("3. LLMã«ã‚ˆã‚‹æ§‹é€ åŒ–æŠ½å‡º")
    print("4. ç•°ãªã‚‹ã‚µã‚¤ãƒˆã§ã®æ‰‹æ³•æ¯”è¼ƒ")
    print("5. ã‚¹ã‚­ãƒ¼ãƒæŸ”è»Ÿæ€§ã®èª¬æ˜")
    print("="*80)
    
    try:
        # ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒãƒ‡ãƒ¢
        await compare_methods()
        
        # ã‚¹ã‚­ãƒ¼ãƒæŸ”è»Ÿæ€§ã®èª¬æ˜
        await demonstrate_schema_flexibility()
        
        # æœ€çµ‚ã‚µãƒãƒªãƒ¼
        print_section("ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†")
        print("âœ… ã™ã¹ã¦ã®ãƒ‡ãƒ¢ãŒå®Œäº†ã—ã¾ã—ãŸ")
        print("\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        print("   - demo_result_*.json - å„ã‚µã‚¤ãƒˆã®æŠ½å‡ºçµæœ")
        print("   - preprocessed_*.txt - å‰å‡¦ç†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿")
        print("\nğŸ“– è©³ç´°ãªæŠ€è¡“ä»•æ§˜ã«ã¤ã„ã¦ã¯ LLM_SCRAPER_DOCUMENTATION.md ã‚’å‚ç…§ã—ã¦ãã ã•ã„")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ãƒ‡ãƒ¢ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸ¬ LLMã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹")
    print("=" * 80)
    
    asyncio.run(main())
    
    print("\nğŸ­ ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†")
    print("ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼")